{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"http://i.imgur.com/sSaOozN.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Integration in Python\n",
    "\n",
    "\n",
    "## Course: Computational Thinking for Governance Analytics<br> Winter 2017\n",
    "\n",
    "### Prof. Jos√© Manuel Magallanes, PhD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done cleaning and formatting your data, you need to integrate it, as, most of the time, you were doing that in different files. Let me get some files first.\n",
    "\n",
    "Let's use these data on [prisons](http://www.doc.wa.gov/information/data/docs/admissions-releases-by-county.pdf) from the State of Washington. The data is in PDF, which is always difficult to process. I tried two interesting services and both failed ([openRefine](http://openrefine.org/) and [tabula](http://tabula.technology/)). This one did a pretty gob job: [pdfTables](https://pdftables.com/). The created file is 'countiesJail.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "folder=\"data\"\n",
    "fileName=\"countiesJail.csv\"\n",
    "fileToRead=os.path.join(folder,fileName)\n",
    "dataJail=pd.read_csv(fileToRead,skiprows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataJail.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the bottom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataJail.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing clear is that I have rows I do not need in the tail. Let's subset getting rid of the last seven rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataJail=dataJail.head(-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's work on the headers. From above, you understand that each column informs the amount of people released and admitted per year. However, that is shown in two rows. There is no easy way to use those rows and make a new header row. So I am going to create the headers. I need to create something like:  \"Admission2006\", \"Release2006\", \"Admission2007\", \"Release2007\",, etc. This is how you do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years=list(range(2006,2016))\n",
    "types=['Admission', 'Release']\n",
    "\n",
    "# first element\n",
    "titlesNew=['county']\n",
    "\n",
    "# adding the other elements\n",
    "titlesNew=titlesNew + [t + str(y) for y in years for t in types]\n",
    "\n",
    "# this is a list comprehension: [t + str(y) for y in years for t in types]\n",
    "\n",
    "## see result:\n",
    "titlesNew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simply change the headers now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataJail.columns=titlesNew\n",
    "\n",
    "# a quick look:\n",
    "dataJail.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that the first row should dissappear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataJail=dataJail.tail(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you get rid of a row that may affect the row names in the data frame you keepin on working, it is better to reset the indexes (positions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataJail.reset_index(drop=True,inplace=True)\n",
    "# You always want to use 'drop=True'\n",
    "# When you use 'inplace=True', you do not need to do this:\n",
    "# dataJail=dataJail.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then\n",
    "dataJail.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns represent counts, let's see what they are for Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataJail.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When python has numbers, it should say _float_; when it says _object_, it means there are strings in the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can not identify visually:\n",
    "dataJail.Admission2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to be sure about the symbol that is provoking that numbers be read as strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using regex\n",
    "import re\n",
    "pattern='\\\\d' #any number\n",
    "nothing=''\n",
    "\n",
    "# \".columns\" gives you all the column names.\n",
    "for column in dataJail.columns[1:]:  # \"[1:]\" avoids the first elemnt (0)\n",
    "    for element in dataJail[column]: \n",
    "        result=re.sub(pattern,nothing,element) #delete any number you find an replace it with...\n",
    "        if result!=nothing:\n",
    "            print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that you know if you have one particular improper character or not, you may find more, and each may require a different treatment. In this case, we are just gonna supress the comma and then convert the value into _float_ (numeric in R): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace ',' with nothing, and convert the column to number:\n",
    "toNum= lambda x: x.str.replace(',','').astype('float')\n",
    "\n",
    "# apply the function to data frame: \n",
    "dataJail[titlesNew[1:]]=dataJail[titlesNew[1:]].apply(toNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is better:\n",
    "dataJail.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I want to know the population per county. These data do not have that information. I need to get information from another file. You can find it in this [website](http://www.ofm.wa.gov/pop/april1/) (the file is \"April 1, 2016 population of cities, towns, and counties used for the allocation of selected state revenues\"). This file is in Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataLink='http://www.ofm.wa.gov/pop/april1/ofm_april1_population_final.xlsx'\n",
    "dataPOP=pd.read_excel(dataLink,0)\n",
    "dataPOP.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reopen again, skipping the first four rows, and keeping 2, 3, 4, and 11th column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPOP=pd.read_excel(dataLink,0,skiprows=4)\n",
    "dataPOP=dataPOP.iloc[:,[1,2,3,10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPOP.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data have a friendly column named _Filter_, which helps knowing the highest administrative area population. As we want the population per county, we would just select the rows with value _1_ in the filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the strategy\n",
    "dataPOP[dataPOP.Filter==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this selection give 39 counties?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataPOP[dataPOP.Filter==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's subset the original data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataPOP=dataPOP[dataPOP.Filter==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the positions:\n",
    "dataPOP.reset_index(drop=True, inplace=True)\n",
    "dataPOP.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the data types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPOP.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The column with the population estimate is  a string. We need to coerce it to numeric, and rename it with a shorter text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPOP.rename(columns={\"2016 Population Estimate \": \"pop2016\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPOP.iloc[:,3]=pd.to_numeric(dataPOP.iloc[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPOP.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show how to integrate these data sets. This will require the use of the function _merge()_.\n",
    "Merging can integrate two data frames at a time. The main requirement is that both data frames have a column with the same unique values, this is the **key** column. \n",
    "\n",
    "Let me subset the _jail_ data just keeping the _county_ and the _admissions_ for 2015:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jail2015=dataJail.iloc[:,[0,19]]\n",
    "# you have \n",
    "jail2015.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, let me see dataPOP without the _Filter_ and _Jurisdiction_ column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPOP=dataPOP.loc[:,['County','pop2016']]\n",
    "# you get\n",
    "dataPOP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The candidates for keys are the columns _county_ and _County_, respectively. These are unique and common values in both data frames. However, thet have different capitalization. In this situation, it is always difficult to know if everything should be upper case ot title case, as either may be required later when we keep integrating. In fact, it would be better to have some universal code instead of text as keys. So let me create another column in dataPOP with the uppercase version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataPOP['county']=dataPOP.County.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the data frames, use 'county' for dataJail and 'county' for dataPOP as the key\n",
    "pd.merge(jail2015,dataPOP, on='county')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. As you see, I have information repeated, but in different format. Let me save the result in jail2015:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jail2015=pd.merge(jail2015,dataPOP, on='county')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a new column, to see what _rate per thousands_ of the current estimated population was recently admitted in jail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jail2015['rateJailPop']=jail2015.Admission2015/(jail2015.pop2016/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jail2015.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could calculate higher order rates, but the county with the minimum population (Garfield) does not go above 10,000 residents, so only thousands are meaningful.\n",
    "\n",
    "The next important step, specially when you have data that can be represented geographically, is to merge the data with a map. We need to install **geopandas**, please erase momentarily the second '#' below (rewrite it later) and install the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+git://github.com/geopandas/geopandas.git\n",
      "  Cloning git://github.com/geopandas/geopandas.git to /private/var/folders/2n/bkfhfqq16r78g3hf7pdj56y40000gn/T/pip-8_tnzbie-build\n",
      "Requirement already satisfied: pandas in /anaconda/envs/governance/lib/python3.6/site-packages (from geopandas==0.3.0+43.g6a72e7e)\n",
      "Requirement already satisfied: shapely in /anaconda/envs/governance/lib/python3.6/site-packages (from geopandas==0.3.0+43.g6a72e7e)\n",
      "Collecting fiona (from geopandas==0.3.0+43.g6a72e7e)\n",
      "  Downloading Fiona-1.7.11-cp36-cp36m-macosx_10_9_intel.macosx_10_9_x86_64.whl (18.8MB)\n",
      "\u001b[K    100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18.8MB 46kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: pyproj in /anaconda/envs/governance/lib/python3.6/site-packages (from geopandas==0.3.0+43.g6a72e7e)\n",
      "Requirement already satisfied: python-dateutil>=2 in /anaconda/envs/governance/lib/python3.6/site-packages (from pandas->geopandas==0.3.0+43.g6a72e7e)\n",
      "Requirement already satisfied: pytz>=2011k in /anaconda/envs/governance/lib/python3.6/site-packages (from pandas->geopandas==0.3.0+43.g6a72e7e)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /anaconda/envs/governance/lib/python3.6/site-packages (from pandas->geopandas==0.3.0+43.g6a72e7e)\n",
      "Collecting munch (from fiona->geopandas==0.3.0+43.g6a72e7e)\n",
      "  Downloading munch-2.2.0.tar.gz\n",
      "Collecting cligj (from fiona->geopandas==0.3.0+43.g6a72e7e)\n",
      "  Using cached cligj-0.4.0-py3-none-any.whl\n",
      "Collecting click-plugins (from fiona->geopandas==0.3.0+43.g6a72e7e)\n",
      "Requirement already satisfied: six in /anaconda/envs/governance/lib/python3.6/site-packages (from fiona->geopandas==0.3.0+43.g6a72e7e)\n",
      "Collecting click>=4.0 (from cligj->fiona->geopandas==0.3.0+43.g6a72e7e)\n",
      "  Downloading click-6.7-py2.py3-none-any.whl (71kB)\n",
      "\u001b[K    100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71kB 2.5MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: munch\n",
      "  Running setup.py bdist_wheel for munch ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/JoseManuel/Library/Caches/pip/wheels/6b/a7/c4/fee97ae4038d2e41e1c862f5940237293b613d2dadd078c0b4\n",
      "Successfully built munch\n",
      "Installing collected packages: munch, click, cligj, click-plugins, fiona, geopandas\n",
      "  Running setup.py install for geopandas ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed click-6.7 click-plugins-1.0.3 cligj-0.4.0 fiona-1.7.11 geopandas-0.3.0+43.g6a72e7e munch-2.2.0\n"
     ]
    }
   ],
   "source": [
    "# You can use pip to install geopandas, but just do this once!!!!!!!\n",
    "\n",
    "!pip install git+git://github.com/geopandas/geopandas.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the map from ALL US counties\n",
    "# it is a shapefile\n",
    "import os\n",
    "folder=\"data/cb_2015_us_county_20m/\"\n",
    "fileName=\"cb_2015_us_county_20m.shp\"\n",
    "\n",
    "fileSHP=os.path.join(folder,fileName) \n",
    "usaCounties = gpd.read_file(fileSHP)\n",
    "\n",
    "hr90 = np.array(ps.open(fileSHP.replace('.shp', '.dbf')).by_col('AWATER'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not GeoDataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8b04dc70ad4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpysal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmaps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmaps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_choropleth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musaCounties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'quantiles'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Greens'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/governance/lib/python3.6/site-packages/pysal/contrib/viz/mapping.py\u001b[0m in \u001b[0;36mplot_choropleth\u001b[0;34m(shp_link, values, type, k, cmap, shp_type, sample_fisher, title, savein, figsize, dpi, alpha)\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m     '''\n\u001b[0;32m--> 994\u001b[0;31m     \u001b[0mshp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshp_link\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshp_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'poly'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0mmap_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_poly_shp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/governance/lib/python3.6/site-packages/pysal/core/FileIO.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, dataPath, mode, dataFormat)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 newCls = object.__new__(cls.__registry[cls.getType(dataPath,\n\u001b[0;32m---> 64\u001b[0;31m                                                                    mode, dataFormat)][mode][0])\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/governance/lib/python3.6/site-packages/pysal/core/FileIO.py\u001b[0m in \u001b[0;36mgetType\u001b[0;34m(dataPath, mode, dataFormat)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataFormat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/governance/lib/python3.6/posixpath.py\u001b[0m in \u001b[0;36msplitext\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb'/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not GeoDataFrame"
     ]
    }
   ],
   "source": [
    "import pysal as ps\n",
    "import numpy as np\n",
    "from pysal.contrib.viz import mapping as maps\n",
    "\n",
    "maps.plot_choropleth(usaCounties, hr90, type='quantiles', cmap='Greens', figsize=(14, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This map includes more counties than needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STATEFP     object\n",
       "COUNTYFP    object\n",
       "COUNTYNS    object\n",
       "AFFGEOID    object\n",
       "GEOID       object\n",
       "NAME        object\n",
       "LSAD        object\n",
       "ALAND        int64\n",
       "AWATER       int64\n",
       "geometry    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usaCounties.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep the counties from WA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Subsetting \n",
    "waCounties=usaCounties[usaCounties['STATEFP']=='53'] \n",
    "waCounties.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column with the county names in title case is the one needed here. I will change that column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jail2015 = jail2015.rename(columns={'County': 'NAME'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the column name will simplify my merge, as the name is the same as the column in the MAP. Let's merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "waCountiesMerge =waCounties.merge(jail2015, on='NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result should be fine:\n",
    "waCountiesMerge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number or rows is the right one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(waCountiesMerge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there is no problem with data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waCountiesMerge.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready for plotting the map. Jupyter need this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input:\n",
    "varToPlot='rateJailPop'\n",
    "colorMap='Reds'\n",
    "numberOfClasses=3\n",
    "title='Inmates per 1000 habitants'\n",
    "\n",
    "# organizing input:\n",
    "args={'column':varToPlot,'cmap':colorMap,'scheme':'Quantiles',\"k\":numberOfClasses,'figsize':(20,13),'legend':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "waCountiesMerge.plot(**args)\n",
    "plt.title(title,{'fontsize': 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you be publishin online, you may like to explore **folium**.\n",
    "\n",
    "Folium requires that we transform our current geodataframe into geojson, let's so that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to_crs(epsg='4326') is needed for the map projection:\n",
    "waGeoJSON = waCountiesMerge.to_crs(epsg='4326').to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we used 3 breaks using a quantile fashion; we need to give the values of the intervals to folium:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is what you need:\n",
    "pd.qcut(waCountiesMerge['rateJailPop'],3).cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate getting those values, we add the argument **netbins** and unpack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories, edges = pd.qcut(waCountiesMerge['rateJailPop'],3, retbins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is what we need!\n",
    "edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the center of the map, we get it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "Longs=[e.x for e in waCountiesMerge['geometry'].centroid]\n",
    "Lats=[e.y for e in waCountiesMerge['geometry'].centroid]\n",
    "mapCenter=[mean(Lats),mean(Longs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "map = folium.Map(mapCenter,zoom_start=7)\n",
    "map.choropleth(geo_str=waGeoJSON, \n",
    "               data=waCountiesMerge,columns=['NAME','rateJailPop'],key_on='feature.properties.NAME',\n",
    "               threshold_scale=list(edges),\n",
    "               fill_color='PuRd', fill_opacity=0.7, line_opacity=0.5,\n",
    "               legend_name='Density', reset=True)\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Homework:\n",
    "\n",
    "Integrate your data sets, and include a map in the merging process."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
